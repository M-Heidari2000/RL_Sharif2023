{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ekcge5vHPpR"
   },
   "source": [
    "# Installations and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJ6K6gUyFP8v",
    "outputId": "b524a6f1-dcce-4602-ca63-69fcbadec9b3"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!pip install 'imageio==2.4.0'\n",
    "!sudo apt-get install -y xvfb ffmpeg\n",
    "!pip3 install gymnasium[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lzUJnJrQEN1r"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import base64\n",
    "import random\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import gymnasium as gym\n",
    "from itertools import count\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csY30Op-HVgu"
   },
   "source": [
    "# Utility functions for rendering evironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B_g_w4-VFqXz"
   },
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "  \n",
    "    video = open(filename,'rb').read()\n",
    "    b64 = base64.b64encode(video)\n",
    "    tag = '''\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>'''.format(b64.decode())\n",
    "    \n",
    "    return IPython.display.HTML(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IH96rj7THfsS"
   },
   "outputs": [],
   "source": [
    "def create_policy_eval_video(env, policy, filename, num_episodes=1, fps=30):\n",
    "  \n",
    "    filename = filename + \".mp4\"\n",
    "    with imageio.get_writer(filename, fps=fps) as video:\n",
    "        for _ in range(num_episodes):\n",
    "            state, info = env.reset()\n",
    "            video.append_data(env.render())\n",
    "            while True:\n",
    "                state = torch.from_numpy(state).unsqueeze(0).to(DEVICE)\n",
    "                action = policy(state)\n",
    "                state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                video.append_data(env.render())\n",
    "                if terminated:\n",
    "                    break\n",
    "    return embed_mp4(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozFEZ_bqH71W"
   },
   "source": [
    "# Replay Memory and Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "84EJUO1JH7h4"
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WD_Av2C7IIZ4"
   },
   "outputs": [],
   "source": [
    "# Complete the Q-Network below. \n",
    "# The Q-Network takes a state as input and the output is a vector so that each element is the q-value for an action.\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # ==================================== Your Code (Begin) ====================================\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(n_observations, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, n_actions)\n",
    "        )\n",
    "        # ==================================== Your Code (End) ====================================\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ==================================== Your Code (Begin) ====================================\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "        # ==================================== Your Code (End) ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_ZW4F6SQeMZ"
   },
   "source": [
    "# Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJ6SfSblT_6f"
   },
   "source": [
    "Now we define 2 policies. We use greedy policy for evaluation and e-greedy during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fokLsyg5Qc41"
   },
   "outputs": [],
   "source": [
    "# This function takes in a state and returns the best action according to your q-network.\n",
    "# Don't forget \"torch.no_grad()\". We don't want gradient flowing through our network. \n",
    "\n",
    "# state shape: (1, state_size) -> output shape: (1, 1)  \n",
    "def greedy_policy(qnet, state):\n",
    "    # ==================================== Your Code (Begin) ====================================\n",
    "    with torch.no_grad():\n",
    "        action_values = qnet(state).squeeze()\n",
    "        return action_values.argmax()\n",
    "    # ==================================== Your Code (End) ===================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_iE-12xgRc2y"
   },
   "outputs": [],
   "source": [
    "# state shape: (1, state_size) -> output shape: (1, 1)\n",
    "# Don't forget \"torch.no_grad()\". We don't want gradient flowing through our network.\n",
    "\n",
    "def e_greedy_policy(qnet, state, current_timestep):\n",
    "    \n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * current_timestep / EPS_DECAY)\n",
    "    # ==================================== Your Code (Begin) ====================================\n",
    "    # With probability \"eps_threshold\" choose a random action \n",
    "    # and with probability 1-\"eps_threshold\" choose the best action according to your Q-Network.\n",
    "    with torch.no_grad():\n",
    "        action_values = qnet(state).squeeze()\n",
    "        random_number = torch.rand(1).item()\n",
    "        if random_number <= eps_threshold:\n",
    "            return torch.randint(len(action_values), size=(1, ))\n",
    "        else:\n",
    "            return action_values.argmax()\n",
    "    # ==================================== Your Code (End) ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PvG1MpOK9mX"
   },
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "5Sc1a-6ZLAE1",
    "outputId": "023fab53-d9e4-45f4-8f04-e362b6b90ba9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n",
      "[swscaler @ 0x55a8db42a280] Warning: data is not aligned! This can lead to a speed loss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <video width=\"640\" height=\"480\" controls>\n",
       "    <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "n_actions = env.action_space.n\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "q_network = DQN(n_observations, n_actions).to(device)\n",
    "target_network = DQN(n_observations, n_actions).to(device)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=LR)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "create_policy_eval_video(env, lambda s: greedy_policy(q_network, s), \"random_agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWq08ZENXx6h"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    'Transition',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "USGbCrKbFusn",
    "outputId": "637da598-4626-4f28-a0ee-0d165a3129fa",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ce4fb7ec734bf18370977ae4c65cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, duration: 20\n",
      "Episode: 1, duration: 11\n",
      "Episode: 2, duration: 16\n",
      "Episode: 3, duration: 32\n",
      "Episode: 4, duration: 27\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m mini_batch \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39msample(BATCH_SIZE)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 4.2 Compute predicted state-action values using q_network\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m q_values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     52\u001b[0m     q_network(x\u001b[38;5;241m.\u001b[39mstate)[x\u001b[38;5;241m.\u001b[39maction] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m mini_batch\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# 4.3 Compute expected state-action values using target_network (Don't forget \"no_grad\" because we don't want gradient through target_network)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[16], line 52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m mini_batch \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39msample(BATCH_SIZE)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 4.2 Compute predicted state-action values using q_network\u001b[39;00m\n\u001b[1;32m     51\u001b[0m q_values \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mq_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m[x\u001b[38;5;241m.\u001b[39maction] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m mini_batch\n\u001b[1;32m     53\u001b[0m ]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# 4.3 Compute expected state-action values using target_network (Don't forget \"no_grad\" because we don't want gradient through target_network)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# ==================================== Your Code (Begin) ====================================\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_relu_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 200\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "episode_returns = []\n",
    "episode_durations = []\n",
    "\n",
    "for i_episode in tqdm(range(num_episodes)):\n",
    "\n",
    "    # ==================================== Your Code (Begin) ====================================\n",
    "    \n",
    "    # 1. Start a new episode\n",
    "    t = 0\n",
    "    G = 0\n",
    "    done = False\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state).to(device)\n",
    "    while not done:\n",
    "        \n",
    "        # 2. Run the environment for 1 step using e-greedy policy\n",
    "        action = e_greedy_policy(qnet=q_network, state=state, current_timestep=i_episode)\n",
    "        next_state, reward, done, _, _ = env.step(action.item())\n",
    "        \n",
    "        # Add reward to the return\n",
    "        G += reward * GAMMA ** t\n",
    "        t += 1\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(next_state).to(device)\n",
    "        \n",
    "        # 3. Add the (state, action, next_state, reward) to replay memory\n",
    "        transition = Transition(\n",
    "            state=state,\n",
    "            action=action,\n",
    "            next_state=next_state,\n",
    "            reward=reward\n",
    "        )\n",
    "        memory.push(transition)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        if len(memory) < 2*BATCH_SIZE:\n",
    "            continue\n",
    "        \n",
    "        # 4. Optimize your q_network for 1 iteration\n",
    "        \n",
    "        # 4.1 Sample one batch from replay memory\n",
    "        mini_batch = memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        # 4.2 Compute predicted state-action values using q_network\n",
    "        q_values = [\n",
    "            q_network(x.state)[x.action] for x in mini_batch\n",
    "        ]\n",
    "        \n",
    "        # 4.3 Compute expected state-action values using target_network (Don't forget \"no_grad\" because we don't want gradient through target_network)\n",
    "        with torch.no_grad():\n",
    "            targets = [\n",
    "                x.reward if x.next_state is None else\n",
    "                x.reward  + GAMMA * target_network(x.next_state).max()\n",
    "                for x in mini_batch\n",
    "            ]\n",
    "            \n",
    "        # 4.4 Compute loss function and optimize q_network for 1 step\n",
    "        loss = sum((target - q)**2 for target, q in zip(targets, q_values)) / len(q_values)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In plac gradient clipping to avoid gradient exploding\n",
    "        torch.nn.utils.clip_grad_value_(q_network.parameters(), 100)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 5. Soft update the weights of target_network\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        # θ   is q_network weights\n",
    "        # θ′  is target_network weights\n",
    "        target_net_state_dict = target_network.state_dict()\n",
    "        q_net_state_dict = q_network.state_dict()\n",
    "        for key in q_net_state_dict:\n",
    "            target_net_state_dict[key] = TAU * q_net_state_dict[key] + (1-TAU) * target_net_state_dict[key]\n",
    "        target_network.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        \n",
    "    # 6. Keep track of the total reward for each episode to plot later\n",
    "    episode_durations.append(t)\n",
    "    episode_returns.append(G)\n",
    "    print(f'Episode: {i_episode}, duration: {t}')\n",
    "    \n",
    "    # ==================================== Your Code (End) ====================================  \n",
    "    \n",
    "print('Complete')\n",
    "plt.plot(range(1, num_episodes+1), episode_durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(episode_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "1s50clnmF_az",
    "outputId": "9ba83866-f303-4b75-bbfb-88fdb2a0e934"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n",
      "[swscaler @ 0x55fbcbdd8280] Warning: data is not aligned! This can lead to a speed loss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <video width=\"640\" height=\"480\" controls>\n",
       "    <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Render trained model\n",
    "\n",
    "create_policy_eval_video(env, lambda s: greedy_policy(q_network, s), \"trained_agent\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
